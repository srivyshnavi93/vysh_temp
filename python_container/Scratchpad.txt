----------------------------------------------------------------------------------------------------
-- TODO: Clean this up
docker run -it --rm -u 0 -p 8080:8080 -p 8081:8081 -p 4040:4040 -p 18080:18080 -v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container --name spark_container apache/spark-py:3.3.1 /bin/bash
---

docker run -it --rm -u 0 -p 7077:7077 -p 8080:8080 -p 4040:4040 -p 18080:18080 \
-v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container \
--name spark_container apache/spark-py:3.3.1 /bin/bash
export SPARK_MASTER_HOST=172.17.0.2
/opt/spark/sbin/start-master.sh

docker run -it --rm -u 0 -p 8081:8081 \
-v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container \
--name spark_container2 apache/spark-py:3.3.1 /bin/bash
export SPARK_MASTER_HOST=172.17.0.2
/opt/spark/sbin/start-worker.sh spark://172.17.0.2:7077

docker run -it --rm -u 0 -p 8888:8888 \
-v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container \
--name spark_container3 apache/spark-py:3.3.1 /bin/bash
apt-get update
pip install jupyterlab
pip install pyspark==3.3.1
pip install findspark
jupyter-lab --no-browser --allow-root --ip 0.0.0.0
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------
Vyshnavi:

docker run -it \
--rm \
--name spark_container \
apache/spark-py:3.3.1 \
/opt/spark/bin/pyspark  


docker run -it \
--name spark_container \
apache/spark-py:3.3.1 \
/opt/spark/bin/pyspark


docker run -it \
--rm \
-u 0 \
-p 8080:8080 \
-p 8081:8081 \
-p 4040:4040 \
-p 18080:18080 \
-v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container \
--name spark_container \
apache/spark-py:3.3.1 \
/bin/bash

docker run -it \
--rm \
-u 0 \
-p 7077:7077 \
-p 8080:8080 \
-p 4040:4040 \
-p 18080:18080 \
-v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container \
--name spark_container_master \
spark_image \
/bin/bash

docker run -it \
--rm \
-u 0 \
-p 8081:8081 \
-v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container \
--name spark_container_worker \
spark_image \
/bin/bash

Note: Worker container port is always 8081 and we are mapping that to 8082 in our host, since first worker has taken host's 8081 port
docker run -it \
--rm \
-u 0 \
-p 8082:8081 \
-v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container \
--name spark_container_worker_2 \
spark_image \
/bin/bash

# Root user is 0, 8080 - master node, 8081 - worker node, 4040 - running spark web UI, 18080 - spark history server
docker run -it \
--rm \
-p 8080:8080 \
-u 0 \
-v /Users/vyshnavi/Documents/GitHub/vysh_temp/python_container/:/opt/spark/work-dir/python_container \
--name spark_container \
apache/spark-py:3.3.1 \
/opt/spark/bin/spark-submit /opt/spark/work-dir/python_container/spark_example.py



docker ps
docker cp python_container/spark_example.py 8e27eb5dc3af:/opt/spark/work-dir/spark_example.py
docker exec -it spark_container bash


docker run -v /Users/vyshnavi/Documents/GitHub/python_container/:/opt/spark/work-dir/python_container apache/spark-py:3.3.1

docker build -t spark_image .

docker run --rm -it --name spark_container spark_image

Running in standalone mode:(running master and worker in same container)
----------------------------
/opt/spark/sbin/start-history-server.sh
/opt/spark/sbin/start-master.sh -h 127.0.0.1
/opt/spark/sbin/start-worker.sh spark://127.0.0.1:7077

Running in standalone mode:(running master and worker in different container)
----------------------------
/opt/spark/sbin/start-history-server.sh
/opt/spark/sbin/start-master.sh [start in master container]
 ==> sbin/start-master.sh
/opt/spark/sbin/start-worker.sh spark://127.0.0.1:7077 [start in worker container]
 ====> sbin/start-worker.sh spark://e0d78f1b5179:7077 [spark://127.0.0.1:7077 will be changing need to check in localhost:8080]

To find out the master IP address
- go to master container and give nano /etc/hosts and you will find the master container IP address in the last line
sbin/start-worker.sh spark://<master_ip_address>:7077
General note: 
---------------
All the docker containers have the same IP address as 127.0.0.1 and two different docker containers cannot communicate with each other.
They can only communicate via host.
So, we need to port map master container listener port(7077) to host port(7077)
And then we need to start worker with host ip address and port 7077

Note:
1.To run spark history server, you need to create "conf", "spark-events" folder in  - root@e4f5596c7798:/opt/spark# ls
2. In "conf" folder need to add spark-defaults.conf file
root@e4f5596c7798:/opt/spark/conf# ls
spark-defaults.conf
3. COntents in spark-defaults.conf file are:

spark.eventLog.enabled true
spark.eventLog.dir file:/opt/spark/spark-events
spark.history.fs.logDirectory file:/opt/spark/spark-events

Note:
Run spark-submit command from worker node.

























